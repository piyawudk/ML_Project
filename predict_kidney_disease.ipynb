{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Predict Kidney Disease**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv('./datasets/chronic_kidney_X.csv')\n",
    "y = pd.read_csv('./datasets/chronic_kidney_y.csv').to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age        9\n",
      "bp        12\n",
      "sg        47\n",
      "al        46\n",
      "su        49\n",
      "bgr       44\n",
      "bu        19\n",
      "sc        17\n",
      "sod       87\n",
      "pot       88\n",
      "hemo      52\n",
      "pcv       71\n",
      "wc       106\n",
      "rc       131\n",
      "rbc      152\n",
      "pc        65\n",
      "pcc        4\n",
      "ba         4\n",
      "htn        2\n",
      "dm         2\n",
      "cad        2\n",
      "appet      1\n",
      "pe         1\n",
      "ane        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some `NaN` value. Let's impute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = (X.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer using median strategy\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [([numeric_feature], SimpleImputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# Apply categorical imputer. The default strategy here is \"mode\"\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine the imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for a full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Define Dictifier class to turn df into dictionary as part of pipeline\n",
    "class Dictifier(BaseEstimator, TransformerMixin):       \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            return X.to_dict(\"records\")\n",
    "        else:\n",
    "            return pd.DataFrame(X).to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold AUC:  0.9992000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "                     (\"featureunion\", numeric_categorical_union), # output in df\n",
    "                     (\"dictifier\", Dictifier()), # convert df into lists\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)), # convert features lists into scipy format\n",
    "                     (\"clf\", xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss',max_depth=3))\n",
    "                    ])\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, X, y, scoring=\"roc_auc\", cv=10)\n",
    "\n",
    "# Print avg. AUC\n",
    "print(\"10-fold AUC: \", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "99.92% is pretty good! Let's see what happened if we tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Best rmse:  0.999599919967984\n",
      "Best model:  Pipeline(steps=[('featureunion',\n",
      "                 FeatureUnion(transformer_list=[('num_mapper',\n",
      "                                                 DataFrameMapper(df_out=True,\n",
      "                                                                 features=[(['age'],\n",
      "                                                                            SimpleImputer(strategy='median')),\n",
      "                                                                           (['bp'],\n",
      "                                                                            SimpleImputer(strategy='median')),\n",
      "                                                                           (['sg'],\n",
      "                                                                            SimpleImputer(strategy='median')),\n",
      "                                                                           (['al'],\n",
      "                                                                            SimpleImputer(strategy='median')),\n",
      "                                                                           (['su'],\n",
      "                                                                            SimpleImputer(strategy='median')),\n",
      "                                                                           (['bgr'],\n",
      "                                                                            SimpleImputer(s...\n",
      "                               importance_type=None, interaction_constraints='',\n",
      "                               learning_rate=0.05, max_delta_step=0,\n",
      "                               max_depth=6, min_child_weight=1, missing=nan,\n",
      "                               monotone_constraints='()', n_estimators=100,\n",
      "                               n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
      "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "                               scale_pos_weight=1, subsample=1,\n",
      "                               tree_method='exact', use_label_encoder=False,\n",
      "                               validate_parameters=1, verbosity=None))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "    'clf__max_depth': np.arange(3, 10, 1),\n",
    "    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,param_distributions=gbm_param_grid,scoring='roc_auc',verbose=1,cv=10)\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X,y)\n",
    "\n",
    "# Compute metrics\n",
    "print(\"Best rmse: \", np.sqrt(np.abs(randomized_roc_auc.best_score_)))\n",
    "print(\"Best model: \", randomized_roc_auc.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0399% improvement in exchange of 40 secs runtime. Is it worth it?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc0d9efdec36a4ba009433853d154b34a3e85cbb49bb5851e9d5bdf304782e05"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
